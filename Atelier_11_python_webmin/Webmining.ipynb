{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Web Mining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-ba001276d91e>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-ba001276d91e>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    1. Introduction\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Sommaire\n",
    "\n",
    "1. Introduction\n",
    "\n",
    "* Principes techniques\n",
    "* Cadre légal\n",
    "\n",
    "2. Page web statique\n",
    "\n",
    "* Les différentes étapes\n",
    "* Exercice: xkcd\n",
    "\n",
    "3. Crawl\n",
    "\n",
    "* Les différentes étapes\n",
    "* Demonstration: marmiton.org\n",
    "\n",
    "4. Pages web dynamiques\n",
    "\n",
    "* Prérequis\n",
    "* Différentes étapes\n",
    "* Démonstration: remplir un formulaire\n",
    "* Démonstration: clicker, scroller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "### Qu'est ce que le webmining?\n",
    "\n",
    "Le web mining (en français: extraction et fouille des données issues du web) est une **technique** d'extraction et de mise en valeur des données appliquées au web.\n",
    "\n",
    "Le principe est simple:\n",
    "* collecter des données en ligne (principalement des pages web)\n",
    "* pour constituer des bases de données\n",
    "* afin de les explorer par la suite.\n",
    "\n",
    "\n",
    "### Principes techniques\n",
    "\n",
    "Le webmining est donc une technique qui repose sur les technologies du web et qui rentre dans la catégorie de l'extraction et analyse de données (Text and Data Mining).\n",
    "\n",
    "Les principales opérations consistent à:\n",
    "-   se connecter à une page web et charger le html (load, connect) dans le cas de page statique c'est une opération  simple, dans le cas de page dynamique c'est une autre paire de manches...\n",
    "-   parcourir la page et extraire les informations cibles   (celles qui nous intéressent: ce qui implique d'avoir identifiés au préalalble les informations qui nous intéresse dans la page soit les balises )\n",
    "- les stocker dans un fichier ou une base de données en les organisant selon ce qui nous arrange pour l'analyse\n",
    "- analyser le résultat de cette extraction. Ici plein de techniques s'appliquent en fonction de l'analyse qu'on veut en faire.\n",
    "  - La première des choses à faire est une **analyse statistique**: on a besoin de vérifier que les données sont complètes et consistantes. La phase classique de mise en cohérence des données\n",
    "  - Une évitable phase de nettoyage est à prévoir: d'expérience, 90% du travail du \"data miner\" est de nettoyer les données, prévoir donc une longue phase qui s'ordonne selon la règle des 80% 20%.\n",
    "  - Ensuite en fonction de l'hypothèse de départ, on mobilise des techniques différentes d'analyse text mining, extraction d'entités nommées, analyses de cooccurence, graphes de relations, classification, etc...\n",
    "\n",
    "### Cadre légal\n",
    "\n",
    "Un petit rappel du cadre légal, le webmining est une technique d'extraction et de mise en valeur de données issues du web elle s'inscrit donc dans le cadre légal du TDM (Text and Data mining).\n",
    "\n",
    "La pratique du text and data mining est complexe. Plusieurs choses à prendre en compte:\n",
    "- législation sur le droit d'auteur et le copyright\n",
    "- législation sur les données à caractère personnels\n",
    "- droit des bases de données et données publiques\n",
    "- législation sur les systèmes informatiques et Internet\n",
    "A ces legislations s'ajoute une clause encore en débat sur l'exception de recherche, débattue lors de la loi pour une république numérique dont les amemdements acceptés sont en vigueur depuis Janvier 2017.\n",
    "\n",
    "##### Droit d'auteur et copyright\n",
    "- Le site en question est soumis au **droit d'auteur**, son accès libre sur Internet ne garantit pas que vous ayez le droit de reproduire tout et partie du site.\n",
    "La réglementation en vigueur est donc soumise à celle du droit d'auteur et au code de la propriété intellectuelle.\n",
    "\n",
    "D'un point de vue technique, votre adresse IP peut être bloquée par l'administrateur du site si vos appels sont trop insistants (banned) et que l'administrateur évalue que vos appels sont une menace pour la stabilité de son service (et oui on peut imaginer que cela s'apparente à une attaque par déni de service et que vous êtes un mechant pirate...)\n",
    "\n",
    "D'un point de vue légal, une demande officielle doit être faite au site en question. En pratique (et c'est MAL), on utilise plein de techniques de pirate pour que nous verrons rapidement à la fin.\n",
    "\n",
    "##### Réglementation du TDM\n",
    "\n",
    "Selon la loi française, le TDM est soumis à une déclaration préalable à la CNIL qui implique de déclarer:\n",
    "\n",
    "- la **nature des données** que vous collectez: données à caractère privé ou non. Si ces données sont à caractère privé un ensemble d'impératif de sécurité sont à mettre en place en conformité avec les recommandation de la CNIL (chiffrement, accès regulé aux données).\n",
    "\n",
    "- le mode de collecte et de **stockage de ses données** :\n",
    "tout comme pour une newsletter ou une inscription à un service, l'utilisateur concerné doit savoir quels usages vont être faire et on doit lui donner un moyen de se désincrire (l'optout est désormais interdit et l'optin par defaut: soit une case à cocher pour votre consentement éclairé)\n",
    "\n",
    " > S'informer sur toutes les recommandations de la [CNIL](https://www.cnil.fr/fr/comprendre-vos-obligations)\n",
    "\n",
    "- **les traitements** qui vont être fait sur ces données:\n",
    "croiser ces données avec d'autres implique de déclarer le mode de valorisation que vous aller en faire. Résolution d'adresse, extraction de relation\n",
    "\n",
    "Il existe depuis janvier 2017 (loi pour une République Numérique), une exception à des fins de recherche, elle ne couvre que les travaux de recherche publique à vocation non-commerciale: il faut pour cela être affilié à un organisme de recherche publique et les conditions d'exercices sont encore flou.\n",
    "\n",
    "C'est un point légal assez complexe que je vous invite à regarder avec la très bonne synthèse sur les enjeux et les questions que cela pose [scinfolex](https://scinfolex.com/2015/07/13/le-statut-juridique-des-donnees-de-la-recherche-entre-droit-des-bases-de-donnees-et-donnees-publiques/)\n",
    "\n",
    "\n",
    "Par ailleurs, à terme et ce n'est pas encore legiféré par décret, ces données devront être transférées à un tiers de confiance (la BNF est présentie pour ce rôle dans le cadre des Archives du web français).\n",
    "\n",
    "Pour etre tranquille, donc il vaut mieux exercer ces talents sur de données en **Open Data**.\n",
    "\n",
    "Ici nous militons activement pour l'Open Science: à savoir l'accès à tous aux savoirs communs pour partage réutiliation et diffusion: cela passe aussi par une législation claire des modalités d'extraction et de manipulation des données du web pour faire science ensemble et de manière transparente.\n",
    "\n",
    "Ensuite dans la pratique, beaucoup de professionnels ne s'embarassent pas de ces précautions légales qui peuvent être durement punies par la loi:\n",
    "A titre indicatif: on peut condamner la personne jusqu'à 3 ans d'emprisonnement et de 45 000 euros d'amende.\n",
    "Mais tout dépend de comment on considère le forfait:\n",
    "- parasitisme économique\n",
    "- vol de données\n",
    "- intrusion dans un système informatique\n",
    "\n",
    "Maintenant vous êtes prévenus !\n",
    "\n",
    "\n",
    "\n",
    "Nous allons d'abord installer dans le système, les modules complémentaires dont nous avons besoin. \n",
    "Dans le terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Installation*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.13.0-py2.py3-none-any.whl\n",
      "Collecting lxml\n",
      "  Using cached lxml-3.7.2-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting BeautifulSoup4\n",
      "  Using cached beautifulsoup4-4.5.3-py2-none-any.whl\n",
      "Installing collected packages: requests, lxml, BeautifulSoup4\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/env_3-5/local/lib/python2.7/site-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/env_3-5/local/lib/python2.7/site-packages/pip/commands/install.py\", line 317, in run\n",
      "    prefix=options.prefix_path,\n",
      "  File \"/env_3-5/local/lib/python2.7/site-packages/pip/req/req_set.py\", line 742, in install\n",
      "    **kwargs\n",
      "  File \"/env_3-5/local/lib/python2.7/site-packages/pip/req/req_install.py\", line 831, in install\n",
      "    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n",
      "  File \"/env_3-5/local/lib/python2.7/site-packages/pip/req/req_install.py\", line 1032, in move_wheel_files\n",
      "    isolated=self.isolated,\n",
      "  File \"/env_3-5/local/lib/python2.7/site-packages/pip/wheel.py\", line 346, in move_wheel_files\n",
      "    clobber(source, lib_dir, True)\n",
      "  File \"/env_3-5/local/lib/python2.7/site-packages/pip/wheel.py\", line 317, in clobber\n",
      "    ensure_dir(destdir)\n",
      "  File \"/env_3-5/local/lib/python2.7/site-packages/pip/utils/__init__.py\", line 83, in ensure_dir\n",
      "    os.makedirs(path)\n",
      "  File \"/env_3-5/lib/python2.7/os.py\", line 157, in makedirs\n",
      "    mkdir(name, mode)\n",
      "OSError: [Errno 13] Permission non accordée: '/env_3-5/lib/python2.7/site-packages/requests'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !/usr/bin/bash\n",
    "\n",
    "! pip install requests lxml BeautifulSoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Télécharger une page web statique\n",
    "\n",
    "### URL => PAGE HTML\n",
    "\n",
    "La première étape est de se connecter à la page web et de télécharger le code source de la page.\n",
    "On va donc à partir d'une url, télécharger une page html. \n",
    "\n",
    "Plusieurs modules pour faire des requêtes avec python existent: \n",
    "le plus simple étant le module [**requests**](http://docs.python-requests.org/en/master/)\n",
    "\n",
    "Ce module télécharge la page demandée  à partir d'une url. Il donne aussi des informations contextuelles sur la requête: le code renvoyé par le serveur, ce qui peut être bien pratique en cas d'erreur.\n",
    "\n",
    "Nous allons écrire une fonction download() qui prend une page web en argument et retourne True et la page **html** si tout c'est bien passé et False, et le code d'erreur HTPP si cela ne s'est pas bien passé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def download(url=\"http://www.marmiton.org/recettes/recette_veloute-chou-fleur-coco-sesame_346982.aspx\"):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return (True, response.text)\n",
    "    else:\n",
    "        return (False, response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#on instancie la fonction avce une url\n",
    "url = \"http://www.marmiton.org/recettes/recette_veloute-chou-fleur-coco-sesame_346982.aspx\"\n",
    "response, page_html = download(url)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Identifier les informations pertinentes\n",
    "### Page HTML => DOM Inspector\n",
    "\n",
    "Nous avons récupéré la page HTML désirée, il s'agit pour le moment de HTML soit un arbre DOM (du texte et des balises HTML). Pour extraire les informations qui nous intéressent: \n",
    "il nous faut détecter les informations qui nous intéresse : dans quelles balises et à quel niveau du document\n",
    "On utilise le DOM Inspector en général (l'exorateur de code de votre navigateur): \n",
    "Ouvrez votre navigateur: tapez F12 et inspecter la page web lister les informations que vous souhaitez extraire.\n",
    "\n",
    "## Parser la page html\n",
    "\n",
    "### Page HTML => DOM \n",
    "\n",
    "On a récupéré la page web, il faut parcourir le code HTML avec un parser. On va utiliser **BeautifulSoup** un module qui donne des méthodes simples pour accéder aux éléments html qui nous intéressent et qui repose sur un parser standard **lxml**. Nous les avons déjà installé nous allons donc mettre le code source de la page HTML dans un parser. Nous allons créer une fonction parse qui prend la page html et parser le code pour pouvoir le manipuler ensuite grace aux méthodes du module BeautifulSoup et extraure les informations dont on a besoin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def parse(page_html):\n",
    "    '''on transforme une page html en une *soupe* avce des tags quon peut rechercher '''\n",
    "    soup = BeautifulSoup(page_html, \"lxml\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#on instancie la fonction en se servant des fonctions déjà écrites\n",
    "\n",
    "url = \"http://www.marmiton.org/recettes/recette_veloute-chou-fleur-coco-sesame_346982.aspx\"\n",
    "response, page_html = download(url)\n",
    "print(response)\n",
    "soup = parse(page_html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Extraire les informations du HTML\n",
    "\n",
    "### DOM  Tags=> elements textuels\n",
    "\n",
    "A cette étape, il est utile de connaitre le HTML. \n",
    "Une fois qu'on a identifié les balises qui contiennent les éléments textuels qu'on veut extraire, on va les chercher et les stocker à l'aide des méthodes de BeautifulSoup find(), findAll(), get().\n",
    "\n",
    "Ici on crée une fonction qui permet d'extraire:\n",
    "- le titre de la page\n",
    "- la liste des ingrédients, \n",
    "- le temps de préparation \n",
    "- le temps de cuission \n",
    "- les instructins de la recette \n",
    "en fonction du type de tag et de la classe de la balise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_recette(soup):\n",
    "    '''cette fonction est spécifique à marmiton'''\n",
    "    titre = soup.find(\"h1\", {\"class\":\"m_title fn\"})\n",
    "    print(\"titre\", titre.text.strip())\n",
    "    #ici on transforme une liste en chaine de caractère séparé par une virgule\n",
    "    ingredients =  \",\".join([n.text for n in soup.findAll(\"a\", {\"class\":\"mrm_al\"})])\n",
    "    print(\"ingredients\", ingredients)\n",
    "    preparation_t = soup.find(\"span\", {\"class\":\"preptime\"}).text\n",
    "    print(\"temps de preparation\", preparation_t)\n",
    "    cuisson_t = soup.find(\"span\", {\"class\":\"cooktime\"}).text\n",
    "    print(\"temps de cuisson\", cuisson_t)\n",
    "    instructions = soup.find(\"div\", {\"class\": \"m_content_recette_todo\"}).text.strip()\n",
    "    print(\"instructions\", instructions)\n",
    "    return (titre, ingredients, preparation_t, cuisson_t, instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "titre \n",
      "\n",
      "Velouté chou-fleur coco sésame\n",
      "\n",
      "\n",
      "ingredients chou-fleur,sésame,sel,poivre,bouillon de légumes\n",
      "temps de preparation 10 \n",
      "temps de cuission 30\n",
      "instructions Préparation de la recette :\n",
      "\r\n",
      "                    Laver et débiter le chou-fleur en petits morceaux.Faire chauffer 1 cuillère à soupe d'huile de sésame et faire revenir à feu vif le chou-fleur pendant 5 min de manière à le colorer.Ajouter le cube de bouillon et couvrir d'eau.Remettre à chauffer sur feu moyen environ 30-40min jusqu'à ce que le chou-fleur soit bien cuit.Sortir du feu, ajouter le lait de coco, le reste d'huile de sésame (1 cuillère à soupe) puis mixer le tout de façon à obtenir un velouté.Assaisonner et déguster !\r\n",
      "                    \n",
      "Remarques :\n",
      "Délicieux avec une tranche de pain au curry grillée ou en croûtons\n"
     ]
    }
   ],
   "source": [
    "#on instancie la fonction en se servant des fonctions déjà écrites\n",
    "\n",
    "url = \"http://www.marmiton.org/recettes/recette_veloute-chou-fleur-coco-sesame_346982.aspx\"\n",
    "response, page_html = download(url)\n",
    "print(response)\n",
    "soup = parse(page_html)\n",
    "recette = extract_recette(soup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Enregistrer et stocker les informations\n",
    "\n",
    "### elements textuels => fichier\n",
    "Une fois ses informatiosn extraite on peut la stocker en écrivant les résultats dans un fichier ou une base de données. Ici pour l'exemple ou va stocker la recette dans un CSV séparé par des tabulations en créant une fonction store_results qui prend les resultats et le nom du fichier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def store_results(results, file_name):\n",
    "    #on ouvre un file descriptor et on met l'option 'a' pour append : soit écrire à la suite\n",
    "    with open(file, \"a\") as f:\n",
    "        #transforme la ligne de resultats en chaine de caractère séparée par une tabulation\n",
    "        line = \"\\t\".join(results)\n",
    "        #ecris dans un fichier la ligne et ajoute un retour à la ligne\n",
    "        f.write(+\"\\n\")\n",
    "    return \n",
    "    \n",
    "#on instancie la fonction en se servant des fonctions déjà écrites\n",
    "url = \"http://www.marmiton.org/recettes/recette_veloute-chou-fleur-coco-sesame_346982.aspx\"\n",
    "response, page_html = download(url)\n",
    "if response is True:\n",
    "    soup = parse(page_html)\n",
    "    recette = extract_recette(soup)\n",
    "    store_results(\"./recettes.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Tout ensemble\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "url = \"\" \n",
    "def extract_data(url):\n",
    "    '''\n",
    "    une fonction qui se connecte, télécharge la page, \n",
    "    parse et extrait les informations\n",
    "    le stockage s'effectue à l'extérieur de cette fonction\n",
    "    '''\n",
    "    #download\n",
    "    response = requests.get(url)\n",
    "    #parse\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "        #extract info\n",
    "        title = soup.find(\"title\").text\n",
    "        links = [n.get(\"href\") for n in soup.findAll(\"a\")]\n",
    "        #return info\n",
    "        return (title, links)\n",
    "infos = extract_data(\"www.lemonde.fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Crawl\n",
    "\n",
    "Les moteurs de recherche utilise la technique du crawl pour indexer les pages web. \n",
    "Un robot (spider) se connecte à une page de départ (seed) télécharge le code source HTML, \n",
    "extrait les informations de la page (titre, texte, mots clés) et les liens disponibles sur cette page et relance la machine.\n",
    "\n",
    "L'araignée spider se promène ainsi de lien en lien jusqu'à ce qu'il n'y ait plus de pages à visiter.\n",
    "\n",
    "L'algorithme est simple:\n",
    "![](./algo.png)\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Pour parcourir un site web entier, il suffit donc par exemple de commencer par la page d'index \n",
    "et de dérouler les liens en enregistrant le parcours de l'araignée à chaque étape.\n",
    "\n",
    "Une implémentation simplifiée..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_infos(url):\n",
    "    '''\n",
    "    moteur minimal: \n",
    "    - stocker les infos d\\'une url \n",
    "    - retourner la liste des urls suivantes\n",
    "    '''\n",
    "    status, html = download(url)\n",
    "    soup = parse(html)\n",
    "    infos = extract_data(url)\n",
    "    store(infos)\n",
    "    return list(set([a.get(\"href\") for a in soup.findAll(\"a\")]))\n",
    "\n",
    "def crawl(seed):\n",
    "    '''parcourir le site'''\n",
    "    deja_visites = []\n",
    "    a_visiter = []\n",
    "    while len(a_visiter) > 0:\n",
    "        for url in a_visiter:\n",
    "            if url not in visites:\n",
    "                next_urls = get_infos(url)\n",
    "                deja_visites.append(url)\n",
    "                next_urls = [url for url in next_urls if url not in deja_visite \n",
    "                                 and url not in a_visiter]\n",
    "                a_visiter.extend(next_url)\n",
    "                \n",
    "            \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidemment on peut ajouter des filtres:\n",
    "    - par exemple ne prendre en compte que les urls du site en question\n",
    "        si l'on souhaite simplement parcourir le site cible en question.\n",
    "        > une idée comment l'implémenter en python?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sites web dynamiques\n",
    "\n",
    "Pour les sites web dynamiques, on a besoin de se faire passer pour un humain \n",
    "avec un navigateur de test pour interagir avec la page et activer le javascript (qui ne s'exécute dans le navigateur).\n",
    "\n",
    "\n",
    "Pour cela on a besoin d'installer des libraires et des modules complémentaires pour émuler un navigateur \n",
    "et de déclarer avec quel système vous voyez la page.\n",
    "\n",
    "En fonction des différentes options dont vous aurez besoin, différents navigateurs peuvent être utilisé:\n",
    "    * PhantomJs le plus simple\n",
    "    * Chrome ou Firefox si vous avez besoin de votre identité et de vos préférences de profil\n",
    "\n",
    "                                                                                 \n",
    "L'installation de ces librairies et modules peut s'avérer extrêmement compliquée \n",
    "et requière à ce jour une version inférieure de Python.\n",
    "(La version Python 2.7)\n",
    "\n",
    "De mon coté, j'utilise selenium qui a une version python mais je recommande le module python qu'on appelle **splinter**\n",
    "qui demande une installation d'un \n",
    "[environnement de développement adapté](http://splinter.readthedocs.io/en/latest/contribute/setting-up-your-development-environment.html).\n",
    "                                                                                 \n",
    "                                                        \n",
    "### Demonstration1 : remplir un formulaire\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
